{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_add_http(url):\n",
    "    \"\"\" \n",
    "    Add https:// on url if it does not contain one\n",
    "    \"\"\"\n",
    "    string_r = r\"^(https://)\"\n",
    "    string_e = re.compile(string_r)\n",
    "    if not string_e.search(url):\n",
    "        url = f\"https://{url}\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_test(url):\n",
    "    \"\"\" Allows to test a url and to know if url has a status code of 200 \"\"\"\n",
    "    \n",
    "    # msg of redirection\n",
    "    redi_text = \"If you are not redirected automatically, follow the www.ioam.de\"\n",
    "    \n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"}\n",
    "\n",
    "    # increase of max_redirect\n",
    "    #session = requests.Session()\n",
    "    #session.max_redirects = 100000000000\n",
    "    try:\n",
    "        url = url_add_http(url)\n",
    "        response = requests.get(url, timeout=60, headers= headers)\n",
    "    except Exception:\n",
    "        # if max_redirects has been exceeded\n",
    "        response = requests.get(url, timeout=60, headers= headers, allow_redirects=False)\n",
    "    \n",
    "    # if the page content contains a redirection message to another site    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tags = soup.find_all(\"body\")\n",
    "    header = soup.find_all(\"head\")\n",
    "    content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "    content = \" \".join(content).split(\" \")[:-1]\n",
    "    redi_text = redi_text.split(\" \")[:-1]\n",
    "\n",
    "    return set(content) != set(redi_text) and response.status_code == 200 and header != []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_url(url):\n",
    "    \"\"\" reformat the domain name to get base url\"\"\"\n",
    "    url_split = url.split(\".\")\n",
    "    return f\"{url_split[-2]}.{url_split[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url):\n",
    "    \"\"\" get url with which the page content will be parsed \"\"\"\n",
    "    if not url_test(url):\n",
    "        url = reformat_url(url)\n",
    "    url = url_add_http(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extracted_from_get_url2(response, url):\n",
    "\n",
    "    return len(response.content)>10 and response.status_code == 200 \n",
    "\n",
    "\n",
    "def get_url2(domains):  # sourcery skip: use-contextlib-suppress\n",
    "    \n",
    "    unique_urls = {}\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"}\n",
    "    for domain in domains:\n",
    "        domain = domain.strip()\n",
    "        try:\n",
    "            url = url_add_http(domain)\n",
    "            response = requests.get(url, timeout=60, headers= headers)\n",
    "\n",
    "            if len(response.content)>10 and response.status_code == 200:\n",
    "                # Get the content of the URL\n",
    "                response = requests.get(url)\n",
    "                content = response.content\n",
    "                 \n",
    "                # Checks if the URL has already been added\n",
    "                if content not in unique_urls.values():\n",
    "                    unique_urls[url] = content \n",
    "             \n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return list(unique_urls.keys()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "\n",
    "def get_url3(domains):    # sourcery skip: use-contextlib-suppress\n",
    "    unique_urls = {}\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "    }\n",
    "\n",
    "    def process_domain(domain):\n",
    "        domain = domain.strip()\n",
    "        try:\n",
    "            url = url_add_http(domain)\n",
    "            response = requests.get(url, timeout=60, headers=headers)\n",
    "\n",
    "            if len(response.content) > 500 and response.status_code == 200:\n",
    "                # Get the content of the URL\n",
    "                response = requests.get(url)\n",
    "                content = response.content\n",
    "\n",
    "                # Checks if the URL has already been added\n",
    "                if content not in unique_urls.values():\n",
    "                    unique_urls[url] = content\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(process_domain, domains)\n",
    "\n",
    "    return list(unique_urls.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12302\n"
     ]
    }
   ],
   "source": [
    "url = \"https://accounts.indianexpress.com\"\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_url(urls):\n",
    "    import requests\n",
    "\n",
    "    # Dictionary to store unique URLs according to their content\n",
    "    unique_urls = {}\n",
    "\n",
    "\n",
    "    for url in urls:\n",
    "        # Get the content of the URL\n",
    "        response = requests.get(url)\n",
    "        content = response.content\n",
    "\n",
    "        # Checks if the URL has already been added\n",
    "        if content not in unique_urls.values():\n",
    "            unique_urls[url] = content\n",
    "\n",
    "    return list(unique_urls.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news.txt\",'r') as f:\n",
    "    data = f.read()\n",
    "    domain = data.split('\\n')[2:]\n",
    "    \n",
    "    \n",
    "urls = get_url3(domain[:200])\n",
    "\n",
    "#urls_news = single_url(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.heise.de', 'https://api.livestrong.com', 'https://bilder1.n-tv.de', 'https://heise.de', 'https://g.msn.com', 'https://cdn.magazin.spiegel.de', 'https://assets.tagesspiegel.de', 'https://content.chip.de', 'https://css.etimg.com', 'https://de.euronews.com', 'https://de.yahoo.com', 'https://efahrer.chip.de', 'https://gutscheine.chip.de', 'https://bilder.t-online.de', 'https://en.wikinews.org', 'https://freemail.t-online.de', 'https://pur.familie.de', 'https://img.welt.de', 'https://mein.tagesspiegel.de', 'https://plus.tagesspiegel.de', 'https://pur.giga.de', 'https://pur.spieletipps.de', 'https://m.tagesspiegel.de', 'https://pur.t-online.de', 'https://news.google.com', 'https://spiele.spiegel.de', 'https://login.t-online.de', 'https://static.euronews.com', 'https://static.up.welt.de', 'https://vergleich.tagesspiegel.de', 'https://web.de', 'https://tarifbestellen.t-online.de', 'https://www.berliner-zeitung.de', 'https://www.ka-news.de', 'https://www.channelpartner.de', 'https://www.chip.de', 'https://www.n-tv.de', 'https://www.computerwoche.de', 'https://www.ndr.de', 'https://www.familie.de', 'https://www.pressebox.de', 'https://www.fr.de', 'https://www.faz.net', 'https://www.spiegel.de', 'https://www.spox.com', 'https://www.t-online.de', 'https://www.spieletipps.de', 'https://www.giga.de', 'https://www.handelsblatt.com', 'https://flipboard.com', 'https://www.speedguide.net', 'https://www1.wdr.de', 'https://jobs.welt.de', 'https://www.yahoo.com', 'https://x.chip.de', 'https://www.zeit.de', 'https://tv.pravda.sk', 'https://media.tv.pravda.sk', 'https://www.abendblatt.de', 'https://www.deutschlandfunk.de', 'https://www.usinenouvelle.com', 'https://poly.augsburger-allgemeine.de', 'https://www.greenpeace.de', 'https://www.niederlausitz-aktuell.de', 'https://markets-data-api-proxy.ft.com', 'https://aboservice.morgenpost.de', 'https://aboshop.morgenpost.de', 'https://repubblica.it', 'https://accounts.hindustantimes.com', 'https://aboshop.berliner-zeitung.de', 'https://accounts.indianexpress.com', 'https://vp.nyt.com', 'https://yahoo.com', 'https://www.wanderlust.co.uk']\n"
     ]
    }
   ],
   "source": [
    "#1m12,1\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [url for url in urls if url is not None]\n",
    "#urls_news = single_url(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.heise.de',\n",
       " 'https://heise.de',\n",
       " 'https://g.msn.com',\n",
       " 'https://api-app.wetteronline.de',\n",
       " 'https://api.livestrong.com',\n",
       " 'https://cdn.magazin.spiegel.de',\n",
       " 'https://bilder1.n-tv.de',\n",
       " 'https://css.etimg.com',\n",
       " 'https://data-7462ea72ec.augsburger-allgemeine.de',\n",
       " 'https://data-92cf33b2ed.faz.net',\n",
       " 'https://data-e3d4300b49.n-tv.de',\n",
       " 'https://data-f1e447fbcf.fr.de',\n",
       " 'https://etelection.indiatimes.com',\n",
       " 'https://apps-cloud.n-tv.de',\n",
       " 'https://bilder.t-online.de',\n",
       " 'https://freemail.t-online.de',\n",
       " 'https://data-af9f3dfb33.zeit.de',\n",
       " 'https://data-fb7f8b3ae8.heise.de',\n",
       " 'https://de.euronews.com',\n",
       " 'https://de.yahoo.com',\n",
       " 'https://efahrer.chip.de',\n",
       " 'https://gutscheine.chip.de',\n",
       " 'https://en.wikinews.org',\n",
       " 'https://flipboard.com',\n",
       " 'https://img.welt.de',\n",
       " 'https://plus.tagesspiegel.de',\n",
       " 'https://login.t-online.de',\n",
       " 'https://m.tagesspiegel.de',\n",
       " 'https://mein.tagesspiegel.de']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [url for url in urls if url is not None]\n",
    "urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
