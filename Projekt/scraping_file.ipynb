{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping:\n",
    "    \"\"\"\n",
    "    this class will allow us to retrieve the content of our web pages\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def web_name(self, url):\n",
    "        \"\"\"\n",
    "        this function allows to obtain the name which is in url, for example for this url https://www.specshop.pl, the name will be specshop\n",
    "\n",
    "        Args:\n",
    "            url (url): url for the web page\n",
    "\n",
    "        Returns:\n",
    "            str: the name that located of the url\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\".join(urlparse(url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def web_title(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page \n",
    "\n",
    "        Returns:\n",
    "            str: title of the page\n",
    "        \"\"\"\n",
    "        return \" \".join(soup.title.contents) if soup.title is not None and \"title\" in soup.html else \" \"\n",
    "\n",
    "    \n",
    "    def web_language(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page \n",
    "\n",
    "        Returns:\n",
    "            str: language of the page\n",
    "        \"\"\"\n",
    "        try:\n",
    "            language = detect(soup.get_text())\n",
    "        except Exception:\n",
    "            language = \"en\"\n",
    "\n",
    "        return language\n",
    "\n",
    " \n",
    " \n",
    "    def web_meta(self, soup):\n",
    "        \"\"\"\n",
    "        gets some meta data from the web page header\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "        Returns:\n",
    "            str: the description of our page in the header\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = soup.find_all(lambda tag: (tag.name=='meta') & (tag.has_attr('name') & tag.has_attr('content')))\n",
    "        \n",
    "        content = [str(tag['content']) for tag in tags if tag['name'] in ['keywords', 'description']]\n",
    "        return \" \".join(content)\n",
    "    \n",
    "    \n",
    "    def web_header(self, soup):\n",
    "        \"\"\"\n",
    "    \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "        Returns:\n",
    "            str: all titles from h1 to h6 of the web page\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "        if not tags:\n",
    "            return \"\"\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        \n",
    "        return \" \".join(content)\n",
    "    \n",
    "    def web_contents(self, soup):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            str: any other element of our html that is not a title, css style, etc...\n",
    "        \"\"\"\n",
    "        \n",
    "        tags_to_ignore = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\",\"h6\", \"noscript\", \"style\", \"script\", \"head\", \"title\", \"meta\", \"[document]\"]\n",
    "        contends = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for word in contends[:50]:\n",
    "            stripped_word = word.strip()\n",
    "        \n",
    "            if (\n",
    "                word.parent.name not in tags_to_ignore\n",
    "                and not isinstance(word, bs4.element.Comment)\n",
    "                and not stripped_word.isnumeric()\n",
    "                and  len(stripped_word) > 0 \n",
    "            ): \n",
    "               \n",
    "                result.append(stripped_word) \n",
    "        return \" \".join(result)\n",
    "        \n",
    "        \n",
    "    def url_contents(self, url):\n",
    "        \"\"\"\n",
    "        return de content of web page\n",
    "        \n",
    "        Args:\n",
    "            url (str): url for the web page\n",
    "\n",
    "        Returns:\n",
    "            Series: content of the web page\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            headers = { \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"}\n",
    "\n",
    "            response = requests.get(url, timeout=60, headers=headers)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                result = {\n",
    "                    \"lang\": \"None\",\n",
    "                    \"url\": \"None\",\n",
    "                    \"website_name\": \"None\",\n",
    "                    \"content_text\": \"None\"\n",
    "                }\n",
    "            else:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "                result = {\n",
    "                    \"lang\": self.web_language(soup),\n",
    "                    \"url\": url,\n",
    "                    \"website_name\": self.web_name(url),\n",
    "                    \"content_text\": self.web_title(soup)+ self.web_meta(soup) + self.web_header(soup) + self.web_contents(soup)\n",
    "                }\n",
    "\n",
    "            return pd.Series(result)\n",
    "        except requests.exceptions.RequestException:\n",
    "            result = {\n",
    "                \"lang\": \"None\",\n",
    "                \"url\": url,\n",
    "                \"website_name\": \"None\",\n",
    "                \"content_text\": \"None\"\n",
    "            }\n",
    "            return pd.Series(result) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "class TextCleaner:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp_dict = {}\n",
    "        \n",
    "    def clean_text(self, document, lang):\n",
    "        \"\"\"\n",
    "        Cleans and normalizes a text according to the given language\n",
    "\n",
    "        Args:\n",
    "            document (str): a text to clean\n",
    "            lang (str): the language of the document\n",
    "\n",
    "        Returns:\n",
    "            str: the normalized text\n",
    "        \"\"\"\n",
    "        \n",
    "        # load the appropriate template if you have not already done it\n",
    "        if lang not in self.nlp_dict:\n",
    "            lang_dict = {\n",
    "                'en': 'en_core_web_sm',\n",
    "                'fr': 'fr_core_news_sm',\n",
    "                'de': 'de_core_news_sm',\n",
    "                # add other languages if necessary\n",
    "            }\n",
    "\n",
    "            self.nlp_dict[lang] = spacy.load(lang_dict.get(lang, 'en_core_web_sm'))\n",
    "\n",
    "        # normalizes the text with the loaded template\n",
    "        doc = self.nlp_dict[lang](document)\n",
    "        tokens = []\n",
    "        exclusion_list = [\"nan\", \"vml\", \"endif\"]\n",
    "\n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum() == False) or token.text in exclusion_list:\n",
    "                continue\n",
    "\n",
    "            # Normalization of the token to lowercase lemmas\n",
    "            token = str(token.lemma_.lower().strip())\n",
    "            tokens.append(token)\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/119321 [00:33<124:14:17,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures # module for running functions in parallel\n",
    "import json\n",
    "import pandas as pd \n",
    "from tqdm import tqdm  # module to create a progress bar\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(domaine, scrap, cleaner):\n",
    "    \"\"\"\n",
    "    (upgrade)Function to extract data from a line in our json file\n",
    "\n",
    "    Args:\n",
    "        domaine (dict): Line of the json file\n",
    "        scrap (Scraping): instance of the Scraping class\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary containing the extracted information\n",
    "    \"\"\"\n",
    "    list_lang = []\n",
    "    try:\n",
    "        dict_domaine = json.loads(domaine)\n",
    "        name = dict_domaine['name']\n",
    "        category = dict_domaine['category']\n",
    "        address = dict_domaine['address']\n",
    "        \n",
    "        content = dict(scrap.url_contents(address))\n",
    "        if content[\"lang\"] != \"None\":\n",
    "            list_lang.append(content['lang'])\n",
    "            content['content_text'] = content['content_text'].strip()\n",
    "            if len(content['content_text'])>=20:\n",
    "                text = cleaner.clean_text(content['content_text'], content['lang'])  # clean up the extracted text\n",
    "                # return a dictionary containing the extracted information\n",
    "                return {'name': name, 'category': category, 'address': address, 'language':content['lang'], 'words': text}\n",
    "    except Exception as e:\n",
    "        print(f\"Error when extracting data for the domain {address}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_dataSet(file_name: str):\n",
    "    \"\"\"\n",
    "    (upgrade)Read a JSON file containing domain names, extract the keywords associated with each address and return a pandas Dataframe containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        file_name (str): json file \n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Panda Dataframe with extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # create an instance of the Scraping class and TextCleaner to extract data from each domain\n",
    "    scrap = Scraping()\n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    # open the JSON file and extract the information\n",
    "    with open(file_name, \"r\") as f:\n",
    "        domaines = f.readlines()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:  # execute functions in parallel with 60 threads\n",
    "        futures = [executor.submit(extract_data, domaine, scrap, cleaner) for domaine in domaines]  # submit tasks\n",
    "        results = []  # initialise the list to store the results\n",
    "        \n",
    "        with tqdm(total=len(domaines)) as pbar:  # create a progress bar to display progress\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result is not None:  # if the task has been executed successfully, add the result to the list\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error when extracting data: {e}\")\n",
    "                pbar.update(1)  # update of the progress bar\n",
    "                \n",
    "    # Create a pandas Dataframe containing the extracted data\n",
    "    data = pd.DataFrame(results)\n",
    "    data['indice'] = range(len(data))\n",
    "    data = data.set_index('indice')\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = get_dataSet('urls.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DataSete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first without threads\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "def get_dataSet(file_name: str):\n",
    "    \"\"\"\n",
    "    Read a JSON file containing domain names, extract the keywords associated with each address and return a pandas Dataframe containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        file_name (str): json file \n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Panda Dataframe with extracted data\n",
    "    \"\"\"\n",
    "       \n",
    "    # creates an instance of the Scraping class   \n",
    "    scrap = Scraping()\n",
    "    clean = TextCleaner()\n",
    "\n",
    "    # open the JSON file and extract the information\n",
    "    with open(file_name, \"r\") as f:\n",
    "        domaines = f.readlines()\n",
    "\n",
    "    # Initialise lists to store data\n",
    "    indices = []\n",
    "    names = []\n",
    "    categories = []\n",
    "    addresses = []\n",
    "    keywords = []\n",
    "\n",
    "    # Iterate through each domain name, extract the keywords associated with the address and store the data in the lists\n",
    "    for index, domaine in tqdm(enumerate(domaines[:50]), total=len(domaines)):\n",
    "        try:\n",
    "            dict_domaine = json.loads(domaine)\n",
    "            name = dict_domaine['name'] \n",
    "            category = dict_domaine['category']\n",
    "            address = dict_domaine['address']\n",
    "            content = dict(scrap.url_content(address))\n",
    "            \n",
    "            if content[\"lang\"] != \"None\":\n",
    "                text = clean.clean_text(content['content_text'], content['lang'])\n",
    "                keywords.append(text)\n",
    "\n",
    "            indices.append(index)\n",
    "            names.append(name)\n",
    "            categories.append(category)\n",
    "            addresses.append(address)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction des données pour le domaine à l'index {index}: {e}\")\n",
    "\n",
    "    # Create a pandas Dataframe containing the extracted data\n",
    "    data = {'name': names, 'category': categories, 'address': addresses, 'words': keywords}\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df = get_dataSet('urls.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraping()\n",
    "clean = TextCleaner()\n",
    "\n",
    "url = \"https://1kino.in\"\n",
    "\n",
    "content = dict(scrap.url_contents(url))\n",
    "\n",
    "#print(content)\n",
    "print()\n",
    "\n",
    "if content[\"lang\"] != \"None\":\n",
    "    print(content[\"lang\"])\n",
    "\n",
    "    text = clean.clean_text(content['content_text'], content['lang'])\n",
    "    print(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
