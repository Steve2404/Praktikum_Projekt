{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from langdetect import detect\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping:\n",
    "    \"\"\"\n",
    "        A class that implements several methods and whose purpose is to retrieve the content of web pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    def url_content(self, url):\n",
    "        \"\"\"\n",
    "        return de content of web page\n",
    "        \n",
    "        Args:\n",
    "            url (str): url for the web page\n",
    "\n",
    "        Returns:\n",
    "            Series: content of the web page\n",
    "        \"\"\"\n",
    "        \n",
    "        # bypass browser restrictions\n",
    "        headers = { \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"}\n",
    "        try:\n",
    "            response = requests.get(url, timeout=60, headers=headers)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                result = {\n",
    "                    \"lang\": \"None\",\n",
    "                    \"url\": \"None\",\n",
    "                    \"website_name\": \"None\",\n",
    "                    \"content_text\": \"None\"\n",
    "                }\n",
    "            else:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "\n",
    "\n",
    "                result = {\n",
    "                    \"lang\": self.get_language(soup),\n",
    "                    \"url\": url,\n",
    "                    \"website_name\": self.get_website_name(url),\n",
    "                    \"content_text\": self.get_title(soup)+ self.get_meta(soup) + self.get_header(soup) + self.get_content(soup)\n",
    "                }\n",
    "\n",
    "            return pd.Series(result)\n",
    "        except requests.exceptions.RequestException:\n",
    "            result = {\n",
    "                \"lang\": \"None\",\n",
    "                \"url\": url,\n",
    "                \"website_name\": \"None\",\n",
    "                \"content_text\": \"None\"\n",
    "            }\n",
    "            return pd.Series(result)\n",
    "    \n",
    "    def get_website_name(self, url):\n",
    "        \"\"\"\n",
    "        this function allows to obtain the name which is in url, for example for this url https://www.specshop.pl, the name will be specshop\n",
    "\n",
    "        Args:\n",
    "            url (url): url for the web page\n",
    "\n",
    "        Returns:\n",
    "            str: the name that located of the url\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\".join(urlparse(url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def get_title(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page \n",
    "\n",
    "        Returns:\n",
    "            str: title of the page\n",
    "        \"\"\"\n",
    "        return \" \".join(soup.title.contents) if soup.title is not None else \"\"\n",
    "\n",
    "    \n",
    "    def get_language(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page \n",
    "\n",
    "        Returns:\n",
    "            str: language of the page\n",
    "        \"\"\"\n",
    "        try:\n",
    "            language = detect(soup.get_text())\n",
    "        except Exception:\n",
    "            language = \"en\"\n",
    "\n",
    "        return language\n",
    "\n",
    " \n",
    " \n",
    "    def get_meta(self, soup):\n",
    "        \"\"\"\n",
    "        gets some meta data from the web page header\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "        Returns:\n",
    "            str: the description of our page in the header\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = soup.find_all(lambda tag: (tag.name=='meta') & (tag.has_attr('name') & tag.has_attr('content')))\n",
    "        \n",
    "        content = [str(tag['content']) for tag in tags if tag['name'] in ['keywords', 'description']]\n",
    "        return \" \".join(content)\n",
    "    \n",
    "    \n",
    "    def get_header(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "        Returns:\n",
    "            str: all titles from h1 to h6 of the web page\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "        if not tags:\n",
    "            return \"\"\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        return \" \".join(content)\n",
    "    \n",
    "    def get_content(self, soup):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            str: any other element of our html that is not a title, css style, etc...\n",
    "        \"\"\"\n",
    "        \n",
    "        tags_to_ignore = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\",\"h6\", \"noscript\", \"style\", \"script\", \"head\", \"title\", \"meta\", \"[document]\"]\n",
    "        contends = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for word in contends:\n",
    "            stripped_word = word.strip()\n",
    "            if (\n",
    "                word.parent.name in tags_to_ignore\n",
    "                or isinstance(word, bs4.element.Comment)\n",
    "                or stripped_word.isnumeric()\n",
    "                or len(stripped_word) <= 0\n",
    "            ):\n",
    "                return \"\"\n",
    "            result.append(stripped_word)\n",
    "        return \" \".join(result)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "class TextCleaner:\n",
    "    \"\"\"\n",
    "    it is the class that will allow us to normalise our texts by deleting unused words\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp_dict = {}\n",
    "        \n",
    "    def clean_text(self, document, lang):\n",
    "        \"\"\"\n",
    "        Cleans and normalizes a text according to the given language\n",
    "\n",
    "        Args:\n",
    "            document (str): a text to clean\n",
    "            lang (str): the language of the document\n",
    "\n",
    "        Returns:\n",
    "            str: the normalized text\n",
    "        \"\"\"\n",
    "        \n",
    "        # load the appropriate template if you have not already done it\n",
    "        if lang not in self.nlp_dict:\n",
    "            lang_dict = {\n",
    "                'en': 'en_core_web_sm',\n",
    "                'fr': 'fr_core_news_sm',\n",
    "                'de': 'de_core_news_sm',\n",
    "                # add other languages if necessary\n",
    "            }\n",
    "\n",
    "            self.nlp_dict[lang] = spacy.load(lang_dict.get(lang, 'en_core_web_sm'))\n",
    "\n",
    "        # normalizes the text with the loaded template\n",
    "        doc = self.nlp_dict[lang](document)\n",
    "        tokens = []\n",
    "        exclusion_list = [\"nan\"]\n",
    "\n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum() == False) or token.text in exclusion_list:\n",
    "                continue\n",
    "\n",
    "            # Normalization of the token to lowercase lemmas\n",
    "            token = str(token.lemma_.lower().strip())\n",
    "            tokens.append(token)\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    " \n",
    "\n",
    "    def decode_text(self, document):\n",
    "        \"\"\"\n",
    "        detects the encoding of the document and decodes it\n",
    "\n",
    "        Args:\n",
    "            document (str): the normalized text\n",
    "\n",
    "        Returns:\n",
    "            str: plain text\n",
    "        \"\"\"\n",
    "        if document is None:\n",
    "            return \"\"\n",
    "        if isinstance(document, str):\n",
    "            document = document.encode('utf-8')\n",
    "        if detected_encoding := chardet.detect(document)['encoding']:\n",
    "            return document.decode(detected_encoding)\n",
    "        else:\n",
    "            return document.decode('utf-8', 'ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:22<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor  # module for running functions in parallel\n",
    "from tqdm import tqdm  # module to create a progress bar\n",
    "\n",
    "\n",
    "def extract_data(domaine, scrap, cleaner):\n",
    "    \"\"\"\n",
    "    (upgrade)Function to extract data from a line in our json file\n",
    "\n",
    "    Args:\n",
    "        domaine (dict): Line of the json file\n",
    "        scrap (Scraping): instance of the Scraping class\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary containing the extracted information\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        dict_domaine = json.loads(domaine)\n",
    "        name = dict_domaine['name']\n",
    "        category = dict_domaine['category']\n",
    "        address = dict_domaine['address']\n",
    "        \n",
    "        content = dict(scrap.url_content(address))\n",
    "        if content[\"lang\"] != \"None\":\n",
    "            content['content_text'] = content['content_text'].strip()\n",
    "            text = cleaner.clean_text(content['content_text'], content['lang'])  # clean up the extracted text\n",
    "            # return a dictionary containing the extracted information\n",
    "            return {'name': name, 'category': category, 'address': address, 'words': text}\n",
    "    except Exception as e:\n",
    "        print(f\"Error when extracting data for the domain {address}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_dataSet(file_name: str):\n",
    "    \"\"\"\n",
    "    (upgrade)Read a JSON file containing domain names, extract the keywords associated with each address and return a pandas Dataframe containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        file_name (str): json file \n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Panda Dataframe with extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # create an instance of the Scraping class and TextCleaner to extract data from each domain\n",
    "    scrap = Scraping()\n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    # open the JSON file and extract the information\n",
    "    with open(file_name, \"r\") as f:\n",
    "        domaines = f.readlines()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=60) as executor:  # execute functions in parallel with 40 threads\n",
    "        futures = [executor.submit(extract_data, domaine, scrap, cleaner) for domaine in domaines[:20]]  # submit tasks\n",
    "        results = []  # initialise the list to store the results\n",
    "        \n",
    "        with tqdm(total=len(futures)) as pbar:  # create a progress bar to display progress\n",
    "            for future in futures:\n",
    "                if future.result():  # if the task has been executed successfully, add the result to the list\n",
    "                    results.append(future.result())\n",
    "                pbar.update(1)  # update of the progress bar\n",
    "                \n",
    "    # Create a pandas Dataframe containing the extracted data\n",
    "    data = pd.DataFrame(results)\n",
    "    data['indice'] = range(len(data))\n",
    "    data = data.set_index('indice')\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = get_dataSet('urls.txt') \n",
    "    df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>address</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indice</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Warez / Software Piracy</td>\n",
       "      <td>https://3mp3.ru</td>\n",
       "      <td>âåóðìáôîï óëáþáôø mp3 íõúùëõ mp3 music free do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Political Extreme / Hate / Discrimination</td>\n",
       "      <td>https://algerie-francaise.org</td>\n",
       "      <td>algerie francaise verite guerre algã rieapre a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://alfasex.net</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Political Extreme / Hate / Discrimination</td>\n",
       "      <td>https://anaporn.com</td>\n",
       "      <td>warn kffn ktkt kcmt fm kmxz fm klpx fm kfma fm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://ahoj.sk</td>\n",
       "      <td>ahojahoj main page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://ageplayteens.com</td>\n",
       "      <td>age play phone sex age play teen age play phon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://4jokerscasino.com</td>\n",
       "      <td>joker gaming joker casinojoker gaming joker ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://2girls1cup.ca</td>\n",
       "      <td>watch girls cup uncensored original videothe o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://adultvideotop.com</td>\n",
       "      <td>adult porn video adult movies adult sex videos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://amateurspankings.com</td>\n",
       "      <td>amateur spanking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Warez / Software Piracy</td>\n",
       "      <td>https://1337x.to</td>\n",
       "      <td>torrent search engine search engine find favor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Warez / Software Piracy</td>\n",
       "      <td>https://adult-sex-games.com</td>\n",
       "      <td>adult sex gamesadult sex game sex game adult g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Political Extreme / Hate / Discrimination</td>\n",
       "      <td>https://altermedia-deutschland.info</td>\n",
       "      <td>website unavailablewebsite unavailable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Warez / Software Piracy</td>\n",
       "      <td>https://1channel.ch</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://animalporn.in</td>\n",
       "      <td>animal pornevery single animal porn video uplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Warez / Software Piracy</td>\n",
       "      <td>https://alpenrammler.com</td>\n",
       "      <td>alpenramml deutsche kostenlos pornos gratis se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Warez / Software Piracy</td>\n",
       "      <td>https://1kino.in</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://animal-lovers.net</td>\n",
       "      <td>animal zoophilia porn worldzoosex animal porn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://1man1jar.org</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Criminal Activities</td>\n",
       "      <td>Illegal Activities</td>\n",
       "      <td>https://allrape.com</td>\n",
       "      <td>rape snuff site visit daily updates extreme po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name                                   category  \\\n",
       "indice                                                                   \n",
       "0       Criminal Activities                    Warez / Software Piracy   \n",
       "1       Criminal Activities  Political Extreme / Hate / Discrimination   \n",
       "2       Criminal Activities                         Illegal Activities   \n",
       "3       Criminal Activities  Political Extreme / Hate / Discrimination   \n",
       "4       Criminal Activities                         Illegal Activities   \n",
       "5       Criminal Activities                         Illegal Activities   \n",
       "6       Criminal Activities                         Illegal Activities   \n",
       "7       Criminal Activities                         Illegal Activities   \n",
       "8       Criminal Activities                         Illegal Activities   \n",
       "9       Criminal Activities                         Illegal Activities   \n",
       "10      Criminal Activities                    Warez / Software Piracy   \n",
       "11      Criminal Activities                    Warez / Software Piracy   \n",
       "12      Criminal Activities  Political Extreme / Hate / Discrimination   \n",
       "13      Criminal Activities                    Warez / Software Piracy   \n",
       "14      Criminal Activities                         Illegal Activities   \n",
       "15      Criminal Activities                    Warez / Software Piracy   \n",
       "16      Criminal Activities                    Warez / Software Piracy   \n",
       "17      Criminal Activities                         Illegal Activities   \n",
       "18      Criminal Activities                         Illegal Activities   \n",
       "19      Criminal Activities                         Illegal Activities   \n",
       "\n",
       "                                    address  \\\n",
       "indice                                        \n",
       "0                           https://3mp3.ru   \n",
       "1             https://algerie-francaise.org   \n",
       "2                       https://alfasex.net   \n",
       "3                       https://anaporn.com   \n",
       "4                           https://ahoj.sk   \n",
       "5                  https://ageplayteens.com   \n",
       "6                 https://4jokerscasino.com   \n",
       "7                     https://2girls1cup.ca   \n",
       "8                 https://adultvideotop.com   \n",
       "9              https://amateurspankings.com   \n",
       "10                         https://1337x.to   \n",
       "11              https://adult-sex-games.com   \n",
       "12      https://altermedia-deutschland.info   \n",
       "13                      https://1channel.ch   \n",
       "14                    https://animalporn.in   \n",
       "15                 https://alpenrammler.com   \n",
       "16                         https://1kino.in   \n",
       "17                https://animal-lovers.net   \n",
       "18                     https://1man1jar.org   \n",
       "19                      https://allrape.com   \n",
       "\n",
       "                                                    words  \n",
       "indice                                                     \n",
       "0       âåóðìáôîï óëáþáôø mp3 íõúùëõ mp3 music free do...  \n",
       "1       algerie francaise verite guerre algã rieapre a...  \n",
       "2                                                          \n",
       "3       warn kffn ktkt kcmt fm kmxz fm klpx fm kfma fm...  \n",
       "4                                      ahojahoj main page  \n",
       "5       age play phone sex age play teen age play phon...  \n",
       "6       joker gaming joker casinojoker gaming joker ca...  \n",
       "7       watch girls cup uncensored original videothe o...  \n",
       "8       adult porn video adult movies adult sex videos...  \n",
       "9                                        amateur spanking  \n",
       "10      torrent search engine search engine find favor...  \n",
       "11      adult sex gamesadult sex game sex game adult g...  \n",
       "12                 website unavailablewebsite unavailable  \n",
       "13                                                         \n",
       "14      animal pornevery single animal porn video uplo...  \n",
       "15      alpenramml deutsche kostenlos pornos gratis se...  \n",
       "16                                                         \n",
       "17      animal zoophilia porn worldzoosex animal porn ...  \n",
       "18                                                         \n",
       "19      rape snuff site visit daily updates extreme po...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indice\n",
       "0    âåóðìáôîï óëáþáôø mp3 íõúùëõ mp3 music free do...\n",
       "1    algerie francaise verite guerre algã rieapre a...\n",
       "2                                                     \n",
       "3    warn kffn ktkt kcmt fm kmxz fm klpx fm kfma fm...\n",
       "4                                   ahojahoj main page\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first without threads\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "def get_dataSet(file_name: str):\n",
    "    \"\"\"\n",
    "    Read a JSON file containing domain names, extract the keywords associated with each address and return a pandas Dataframe containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        file_name (str): json file \n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Panda Dataframe with extracted data\n",
    "    \"\"\"\n",
    "       \n",
    "    # creates an instance of the Scraping class   \n",
    "    scrap = Scraping()\n",
    "\n",
    "    # open the JSON file and extract the information\n",
    "    with open(file_name, \"r\") as f:\n",
    "        domaines = f.readlines()\n",
    "\n",
    "    # Initialise lists to store data\n",
    "    indices = []\n",
    "    names = []\n",
    "    categories = []\n",
    "    addresses = []\n",
    "    keywords = []\n",
    "\n",
    "    # Iterate through each domain name, extract the keywords associated with the address and store the data in the lists\n",
    "    for index, domaine in tqdm(enumerate(domaines[:50]), total=len(domaines)):\n",
    "        try:\n",
    "            dict_domaine = json.loads(domaine)\n",
    "            name = dict_domaine['name'] \n",
    "            category = dict_domaine['category']\n",
    "            address = dict_domaine['address']\n",
    "            content = dict(scrap.url_content(address))\n",
    "            \n",
    "            if content[\"lang\"] != \"None\":\n",
    "                text = clean_text(content['content_text'], content['lang'])\n",
    "                keywords.append(text)\n",
    "\n",
    "            indices.append(index)\n",
    "            names.append(name)\n",
    "            categories.append(category)\n",
    "            addresses.append(address)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction des données pour le domaine à l'index {index}: {e}\")\n",
    "\n",
    "    # Create a pandas dataframe containing the extracted data\n",
    "    data = {'indice': indices, 'name': names, 'category': categories, 'address': addresses, 'words': keywords}\n",
    "    return pd.DataFrame(data)\n",
    "if __name__ == '__main__':\n",
    "    df = get_dataSet('urls.txt')\n",
    "    df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraping()\n",
    "url = \"https://alfasex.net\"\n",
    "\n",
    "content = dict(scrap.url_content(url))\n",
    "\n",
    "#print(content)\n",
    "print()\n",
    "\n",
    "if content[\"lang\"] != \"None\":\n",
    "    text = clean_text(content['content_text'], content['lang'])\n",
    "    print(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
