{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from langdetect import detect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping:\n",
    "    \"\"\"\n",
    "    cette classe va nous permettre de récupérer le contenu de nos pages webs\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def web_name(self, url):\n",
    "        \"\"\"\n",
    "        this function allows to obtain the name which is in url, for example for this url https://www.specshop.pl, the name will be specshop\n",
    "\n",
    "        Args:\n",
    "            url (url): url for the web page\n",
    "\n",
    "        Returns:\n",
    "            str: the name that located of the url\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\".join(urlparse(url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def web_title(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page \n",
    "\n",
    "        Returns:\n",
    "            str: title of the page\n",
    "        \"\"\"\n",
    "        return \" \".join(soup.title.contents) if soup.title is not None else \"\"\n",
    "\n",
    "    \n",
    "    def web_language(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page \n",
    "\n",
    "        Returns:\n",
    "            str: language of the page\n",
    "        \"\"\"\n",
    "        try:\n",
    "            language = detect(soup.get_text())\n",
    "        except Exception:\n",
    "            language = \"en\"\n",
    "\n",
    "        return language\n",
    "\n",
    " \n",
    " \n",
    "    def web_meta(self, soup):\n",
    "        \"\"\"\n",
    "        gets some meta data from the web page header\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "        Returns:\n",
    "            str: the description of our page in the header\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = soup.find_all(lambda tag: (tag.name=='meta') & (tag.has_attr('name') & tag.has_attr('content')))\n",
    "        \n",
    "        content = [str(tag['content']) for tag in tags if tag['name'] in ['keywords', 'description']]\n",
    "        return \" \".join(content)\n",
    "    \n",
    "    \n",
    "    def web_header(self, soup):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "        Returns:\n",
    "            str: all titles from h1 to h6 of the web page\n",
    "        \"\"\"\n",
    "        \n",
    "        tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "        if not tags:\n",
    "            return \"\"\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        return \" \".join(content)\n",
    "    \n",
    "    def web_contents(self, soup):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): get the whole html parse document of the web page\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            str: any other element of our html that is not a title, css style, etc...\n",
    "        \"\"\"\n",
    "        \n",
    "        tags_to_ignore = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\",\"h6\", \"noscript\", \"style\", \"script\", \"head\", \"title\", \"meta\", \"[document]\"]\n",
    "        contends = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for word in contends:\n",
    "            stripped_word = word.strip()\n",
    "            if (\n",
    "                word.parent.name in tags_to_ignore\n",
    "                or isinstance(word, bs4.element.Comment)\n",
    "                or stripped_word.isnumeric()\n",
    "                or len(stripped_word) <= 0\n",
    "            ):\n",
    "                return \"\"\n",
    "            result.append(stripped_word)\n",
    "        return \" \".join(result)\n",
    "        \n",
    "        \n",
    "    def url_contents(self, url):\n",
    "        \"\"\"\n",
    "        return de content of web page\n",
    "        \n",
    "        Args:\n",
    "            url (str): url for the web page\n",
    "\n",
    "        Returns:\n",
    "            Series: content of the web page\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            headers = { \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0\"}\n",
    "\n",
    "            response = requests.get(url, timeout=60, headers=headers)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                result = {\n",
    "                    \"lang\": \"None\",\n",
    "                    \"url\": \"None\",\n",
    "                    \"website_name\": \"None\",\n",
    "                    \"content_text\": \"None\"\n",
    "                }\n",
    "            else:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "                result = {\n",
    "                    \"lang\": self.get_language(soup),\n",
    "                    \"url\": url,\n",
    "                    \"website_name\": self.get_name(url),\n",
    "                    \"content_text\": self.web_title(soup)+ self.web_meta(soup) + self.web_header(soup) + self.web_contents(soup)\n",
    "                }\n",
    "\n",
    "            return pd.Series(result)\n",
    "        except requests.exceptions.RequestException:\n",
    "            result = {\n",
    "                \"lang\": \"None\",\n",
    "                \"url\": url,\n",
    "                \"website_name\": \"None\",\n",
    "                \"content_text\": \"None\"\n",
    "            }\n",
    "            return pd.Series(result) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "class TextCleaner:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp_dict = {}\n",
    "        \n",
    "    def clean_text(self, document, lang):\n",
    "        \"\"\"\n",
    "        Cleans and normalizes a text according to the given language\n",
    "\n",
    "        Args:\n",
    "            document (str): a text to clean\n",
    "            lang (str): the language of the document\n",
    "\n",
    "        Returns:\n",
    "            str: the normalized text\n",
    "        \"\"\"\n",
    "        \n",
    "        # load the appropriate template if you have not already done it\n",
    "        if lang not in self.nlp_dict:\n",
    "            lang_dict = {\n",
    "                'en': 'en_core_web_sm',\n",
    "                'fr': 'fr_core_news_sm',\n",
    "                'de': 'de_core_news_sm',\n",
    "                # add other languages if necessary\n",
    "            }\n",
    "\n",
    "            self.nlp_dict[lang] = spacy.load(lang_dict.get(lang, 'en_core_web_sm'))\n",
    "\n",
    "        # normalizes the text with the loaded template\n",
    "        doc = self.nlp_dict[lang](document)\n",
    "        tokens = []\n",
    "        exclusion_list = [\"nan\"]\n",
    "\n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum() == False) or token.text in exclusion_list:\n",
    "                continue\n",
    "\n",
    "            # Normalization of the token to lowercase lemmas\n",
    "            token = str(token.lemma_.lower().strip())\n",
    "            tokens.append(token)\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/119321 [01:16<29:24:49,  1.13it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "  1%|          | 1307/119321 [05:33<15:29:54,  2.12it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 1975/119321 [07:24<11:11:02,  2.91it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 3198/119321 [10:51<5:05:34,  6.33it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 4653/119321 [15:16<10:13:14,  3.12it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 5075/119321 [16:23<11:53:52,  2.67it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 6289/119321 [19:53<2:50:03, 11.08it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "  6%|▋         | 7674/119321 [24:13<10:02:17,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://netian.com: sequence item 1: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7777/119321 [24:28<5:57:27,  5.20it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 10%|▉         | 11764/119321 [37:15<9:56:09,  3.01it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 10%|█         | 12004/119321 [37:45<5:13:46,  5.70it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 15924/119321 [49:47<9:36:28,  2.99it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 27169/119321 [1:25:10<9:18:39,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://perfectiondistributing.com: sequence item 1: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 27336/119321 [1:25:40<9:17:22,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://pointyhatdesigns.com: sequence item 0: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 31138/119321 [1:38:17<4:52:12,  5.03it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 30%|██▉       | 35571/119321 [1:50:18<5:30:29,  4.22it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 30%|███       | 36119/119321 [1:51:42<5:38:44,  4.09it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 37701/119321 [1:55:53<7:05:24,  3.20it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 38179/119321 [1:57:05<2:58:11,  7.59it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 43695/119321 [2:12:52<5:20:05,  3.94it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 43788/119321 [2:13:05<7:12:52,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://hotxhamsterporn.com: sequence item 0: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 50016/119321 [2:30:35<11:05:07,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://ohgodimcumming.com: sequence item 0: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 52208/119321 [2:36:35<5:29:53,  3.39it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 64172/119321 [3:10:24<5:56:32,  2.58it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 66194/119321 [3:16:44<6:18:56,  2.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://cut-the-knot.org: sequence item 1: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 67405/119321 [3:20:18<3:20:52,  4.31it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 71650/119321 [3:33:42<11:33:31,  1.15it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 72297/119321 [3:35:33<3:29:23,  3.74it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 72381/119321 [3:35:48<3:23:05,  3.85it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 72544/119321 [3:36:12<3:00:57,  4.31it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 72545/119321 [3:36:12<3:32:35,  3.67it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 73905/119321 [3:40:19<4:45:27,  2.65it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 77842/119321 [3:52:10<40:39, 17.01it/s]   /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 81182/119321 [4:02:39<3:54:56,  2.71it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 82790/119321 [4:07:30<2:13:32,  4.56it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 70%|██████▉   | 83117/119321 [4:08:28<4:11:44,  2.40it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 83718/119321 [4:10:18<53:08, 11.17it/s]   /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 83954/119321 [4:10:53<3:27:51,  2.84it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 71%|███████▏  | 85089/119321 [4:14:22<1:30:43,  6.29it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 86239/119321 [4:17:37<4:01:36,  2.28it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 74%|███████▎  | 87980/119321 [4:22:41<4:31:48,  1.92it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 90077/119321 [4:28:42<1:48:10,  4.51it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 91315/119321 [4:32:36<59:51,  7.80it/s]   /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 91391/119321 [4:32:51<2:38:44,  2.93it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 91392/119321 [4:32:51<2:45:47,  2.81it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 91393/119321 [4:32:53<4:27:47,  1.74it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 91990/119321 [4:34:43<1:54:35,  3.98it/s] /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 79%|███████▉  | 94440/119321 [4:42:25<1:21:34,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when extracting data for the domain https://mitsuwa-i.com: sequence item 1: expected str instance, Tag found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 96364/119321 [4:47:51<1:24:56,  4.50it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 98315/119321 [4:53:30<41:17,  8.48it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 83%|████████▎ | 98679/119321 [4:54:33<1:04:29,  5.33it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 100234/119321 [4:59:02<1:22:04,  3.88it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 100425/119321 [4:59:38<2:01:35,  2.59it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 100522/119321 [4:59:48<1:33:03,  3.37it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 85%|████████▍ | 101123/119321 [5:01:31<1:27:20,  3.47it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 101987/119321 [5:03:58<2:30:48,  1.92it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 86%|████████▌ | 102732/119321 [5:06:02<1:24:54,  3.26it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 105040/119321 [5:12:37<1:30:26,  2.63it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 107413/119321 [5:19:47<14:46, 13.44it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 108311/119321 [5:22:28<1:10:09,  2.62it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 111437/119321 [5:31:28<06:09, 21.32it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 112363/119321 [5:34:37<22:21,  5.19it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 113098/119321 [5:37:09<27:59,  3.71it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 113100/119321 [5:37:13<1:00:32,  1.71it/s]/home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 114279/119321 [5:41:05<13:52,  6.06it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 96%|█████████▋| 115064/119321 [5:43:52<12:13,  5.80it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 116592/119321 [5:48:48<09:24,  4.83it/s]  /home/ljatsa/.local/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "100%|██████████| 119321/119321 [5:58:26<00:00,  5.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import concurrent.futures\n",
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor  # module for running functions in parallel\n",
    "from tqdm import tqdm  # module to create a progress bar\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(domaine, scrap, cleaner):\n",
    "    \"\"\"\n",
    "    (upgrade)Function to extract data from a line in our json file\n",
    "\n",
    "    Args:\n",
    "        domaine (dict): Line of the json file\n",
    "        scrap (Scraping): instance of the Scraping class\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary containing the extracted information\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        dict_domaine = json.loads(domaine)\n",
    "        name = dict_domaine['name']\n",
    "        category = dict_domaine['category']\n",
    "        address = dict_domaine['address']\n",
    "        \n",
    "        content = dict(scrap.url_contents(address))\n",
    "        if content[\"lang\"] != \"None\":\n",
    "            content['content_text'] = content['content_text'].strip()\n",
    "            if len(content['content_text'])>=20:\n",
    "                text = cleaner.clean_text(content['content_text'], content['lang'])  # clean up the extracted text\n",
    "                # return a dictionary containing the extracted information\n",
    "                return {'name': name, 'category': category, 'address': address, 'words': text}\n",
    "    except Exception as e:\n",
    "        print(f\"Error when extracting data for the domain {address}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_dataSet(file_name: str):\n",
    "    \"\"\"\n",
    "    (upgrade)Read a JSON file containing domain names, extract the keywords associated with each address and return a pandas Dataframe containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        file_name (str): json file \n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Panda Dataframe with extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # create an instance of the Scraping class and TextCleaner to extract data from each domain\n",
    "    scrap = Scraping()\n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    # open the JSON file and extract the information\n",
    "    with open(file_name, \"r\") as f:\n",
    "        domaines = f.readlines()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:  # execute functions in parallel with 60 threads\n",
    "        futures = [executor.submit(extract_data, domaine, scrap, cleaner) for domaine in domaines]  # submit tasks\n",
    "        results = []  # initialise the list to store the results\n",
    "        \n",
    "        with tqdm(total=len(domaines)) as pbar:  # create a progress bar to display progress\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result is not None:  # if the task has been executed successfully, add the result to the list\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error when extracting data: {e}\")\n",
    "                pbar.update(1)  # update of the progress bar\n",
    "                \n",
    "    # Create a pandas Dataframe containing the extracted data\n",
    "    data = pd.DataFrame(results)\n",
    "    data['indice'] = range(len(data))\n",
    "    data = data.set_index('indice')\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = get_dataSet('urls.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Illegal Activities', 'Warez / Software Piracy',\n",
       "       'Computer Crime / Hacking',\n",
       "       'Computer Crime / Hacking, Illegal Activities, Warez / Software Piracy',\n",
       "       'Political Extreme / Hate / Discrimination',\n",
       "       'Illegal Activities, Warez / Software Piracy',\n",
       "       'Computer Crime / Hacking, Warez / Software Piracy',\n",
       "       'Illegal Activities, Political Extreme / Hate / Discrimination',\n",
       "       'Warez / Software Piracy, Computer Crime / Hacking, Illegal Activities',\n",
       "       'Political Extreme / Hate / Discrimination, Illegal Activities',\n",
       "       'Warez / Software Piracy, Illegal Activities',\n",
       "       'Warez / Software Piracy, Computer Crime / Hacking',\n",
       "       'Computer Crime / Hacking, Warez / Software Piracy, Illegal Activities',\n",
       "       'Warez / Software Piracy, Political Extreme / Hate / Discrimination',\n",
       "       'Alcohol', 'Tobacco', 'Self-Help / Addiction',\n",
       "       'Illegal Drugs, Tobacco', 'Illegal Drugs',\n",
       "       'Music / Radio Broadcast', 'Humour / Cartoons',\n",
       "       'Cinema / Television', 'Arts / Museums / Theatres',\n",
       "       'Recreational Facilities / Theme Parks',\n",
       "       'Cinema / Television, Music / Radio Broadcast',\n",
       "       'Humour / Cartoons, Cinema / Television, Music / Radio Broadcast',\n",
       "       'Literature / Books',\n",
       "       'Music / Radio Broadcast, Literature / Books, Cinema / Television',\n",
       "       'Music / Radio Broadcast, Literature / Books',\n",
       "       'Cinema / Television, Humour / Cartoons',\n",
       "       'Humour / Cartoons, Cinema / Television',\n",
       "       'Music / Radio Broadcast, Cinema / Television',\n",
       "       'Music / Radio Broadcast, Cinema / Television, Humour / Cartoons',\n",
       "       'Humour / Cartoons, Literature / Books',\n",
       "       'Recreational Facilities / Theme Parks, Cinema / Television',\n",
       "       'Music / Radio Broadcast, Arts / Museums / Theatres, Cinema / Television',\n",
       "       'Cinema / Television, Arts / Museums / Theatres',\n",
       "       'Cinema / Television, Music / Radio Broadcast, Humour / Cartoons',\n",
       "       'Humour / Cartoons, Music / Radio Broadcast, Cinema / Television',\n",
       "       'Music / Radio Broadcast, Humour / Cartoons, Cinema / Television',\n",
       "       'Cinema / Television, Humour / Cartoons, Music / Radio Broadcast',\n",
       "       'Music / Radio Broadcast, Humour / Cartoons',\n",
       "       'Literature / Books, Arts / Museums / Theatres',\n",
       "       'Literature / Books, Cinema / Television, Music / Radio Broadcast',\n",
       "       'Literature / Books, Recreational Facilities / Theme Parks',\n",
       "       'Recreational Facilities / Theme Parks, Music / Radio Broadcast',\n",
       "       'Literature / Books, Music / Radio Broadcast',\n",
       "       'Humour / Cartoons, Music / Radio Broadcast',\n",
       "       'Literature / Books, Cinema / Television',\n",
       "       'Recreational Facilities / Theme Parks, Arts / Museums / Theatres',\n",
       "       'Arts / Museums / Theatres, Music / Radio Broadcast',\n",
       "       'Music / Radio Broadcast, Arts / Museums / Theatres',\n",
       "       'Financial Services / Insurance / Real Estate, Banking, Brokers / Stock Exchange',\n",
       "       'Brokers / Stock Exchange, Financial Services / Insurance / Real Estate, Banking',\n",
       "       'Banking', 'Financial Services / Insurance / Real Estate',\n",
       "       'Brokers / Stock Exchange, Financial Services / Insurance / Real Estate',\n",
       "       'Financial Services / Insurance / Real Estate, Banking',\n",
       "       'Brokers / Stock Exchange, Banking, Financial Services / Insurance / Real Estate',\n",
       "       'Brokers / Stock Exchange, Banking',\n",
       "       'Banking, Financial Services / Insurance / Real Estate, Brokers / Stock Exchange',\n",
       "       'Financial Services / Insurance / Real Estate, Brokers / Stock Exchange, Banking',\n",
       "       'Banking, Financial Services / Insurance / Real Estate',\n",
       "       'Banking, Brokers / Stock Exchange, Financial Services / Insurance / Real Estate',\n",
       "       'Brokers / Stock Exchange', 'Banking, Brokers / Stock Exchange',\n",
       "       'Financial Services / Insurance / Real Estate, Brokers / Stock Exchange',\n",
       "       'Computer Games', 'Gambling / Lottery', 'Toys',\n",
       "       'Computer Games, Gambling / Lottery', 'Computer Games, Toys',\n",
       "       'Gambling / Lottery, Computer Games', 'Toys, Computer Games',\n",
       "       'Webmail / Unified Messaging', 'Blogs / Bulletin Boards',\n",
       "       'Search Engines / Web Catalogues / Portals', 'News / Magazines',\n",
       "       'Digital Postcards', 'Chat',\n",
       "       'Search Engines / Web Catalogues / Portals, Blogs / Bulletin Boards',\n",
       "       'Mobile Telephony', 'News / Magazines, Blogs / Bulletin Boards',\n",
       "       'Blogs / Bulletin Boards, News / Magazines',\n",
       "       'News / Magazines, Search Engines / Web Catalogues / Portals, Blogs / Bulletin Boards',\n",
       "       'Blogs / Bulletin Boards, Search Engines / Web Catalogues / Portals',\n",
       "       'Blogs / Bulletin Boards, Chat',\n",
       "       'News / Magazines, Search Engines / Web Catalogues / Portals',\n",
       "       'Search Engines / Web Catalogues / Portals, News / Magazines',\n",
       "       'Search Engines / Web Catalogues / Portals, Blogs / Bulletin Boards, News / Magazines',\n",
       "       'Search Engines / Web Catalogues / Portals, News / Magazines, Blogs / Bulletin Boards',\n",
       "       'Mobile Telephony, Blogs / Bulletin Boards',\n",
       "       'Blogs / Bulletin Boards, News / Magazines, Search Engines / Web Catalogues / Portals',\n",
       "       'Chat, Instant Messaging',\n",
       "       'Blogs / Bulletin Boards, Search Engines / Web Catalogues / Portals, News / Magazines',\n",
       "       'Instant Messaging, Chat', 'Chat, Blogs / Bulletin Boards',\n",
       "       'News / Magazines, Webmail / Unified Messaging, Chat, Mobile Telephony',\n",
       "       'Chat, Search Engines / Web Catalogues / Portals',\n",
       "       'Instant Messaging', 'Blogs / Bulletin Boards, Mobile Telephony',\n",
       "       'News / Magazines, Blogs / Bulletin Boards, Search Engines / Web Catalogues / Portals',\n",
       "       'Blogs / Bulletin Boards, Digital Postcards',\n",
       "       'Search Engines / Web Catalogues / Portals, Chat',\n",
       "       'Search Engines / Web Catalogues / Portals, Webmail / Unified Messaging, News / Magazines',\n",
       "       'Webmail / Unified Messaging, Chat',\n",
       "       'Instant Messaging, Mobile Telephony',\n",
       "       'Search Engines / Web Catalogues / Portals, Mobile Telephony',\n",
       "       'Search Engines / Web Catalogues / Portals, Digital Postcards',\n",
       "       'Blogs / Bulletin Boards, Mobile Telephony, News / Magazines',\n",
       "       'News / Magazines, Webmail / Unified Messaging, Search Engines / Web Catalogues / Portals',\n",
       "       'News / Magazines, Blogs / Bulletin Boards, Chat',\n",
       "       'Search Engines / Web Catalogues / Portals, News / Magazines, Webmail / Unified Messaging',\n",
       "       'Blogs / Bulletin Boards, Search Engines / Web Catalogues / Portals, Mobile Telephony',\n",
       "       'Webmail / Unified Messaging, News / Magazines, Blogs / Bulletin Boards',\n",
       "       'Mobile Telephony, Instant Messaging', 'Software / Hardware',\n",
       "       'Anonymous Proxies', 'Communication Services',\n",
       "       'Anonymous Proxies, IT Security / IT Information, Software / Hardware',\n",
       "       'IT Security / IT Information', 'Web Site Translation',\n",
       "       'IT Security / IT Information, Software / Hardware',\n",
       "       'Software / Hardware, Communication Services',\n",
       "       'Software / Hardware, IT Security / IT Information',\n",
       "       'Communication Services, Software / Hardware',\n",
       "       'Anonymous Proxies, IT Security / IT Information',\n",
       "       'IT Security / IT Information, Communication Services',\n",
       "       'Software / Hardware, Anonymous Proxies',\n",
       "       'IT Security / IT Information, Anonymous Proxies',\n",
       "       'IT Security / IT Information, Software / Hardware, Anonymous Proxies',\n",
       "       'Anonymous Proxies, Software / Hardware',\n",
       "       'Communication Services, IT Security / IT Information, Software / Hardware',\n",
       "       'Software / Hardware, Web Site Translation',\n",
       "       'Communication Services, IT Security / IT Information',\n",
       "       'Job Search', 'Restaurants / Entertainment Venues', 'Travel',\n",
       "       'Fashion / Cosmetics / Jewellery', 'Sports', 'Dating',\n",
       "       'Architecture / Construction / Furniture',\n",
       "       'Environment / Climate / Pets',\n",
       "       'Sports, Fashion / Cosmetics / Jewellery',\n",
       "       'Fashion / Cosmetics / Jewellery, Sports',\n",
       "       'Travel, Restaurants / Entertainment Venues',\n",
       "       'Environment / Climate / Pets, Travel', 'Travel, Sports',\n",
       "       'Sports, Travel',\n",
       "       'Fashion / Cosmetics / Jewellery, Architecture / Construction / Furniture',\n",
       "       'Environment / Climate / Pets, Architecture / Construction / Furniture',\n",
       "       'Dating, Travel', 'Restaurants / Entertainment Venues, Travel',\n",
       "       'Architecture / Construction / Furniture, Environment / Climate / Pets',\n",
       "       'Health', 'Abortion', 'Health, Abortion', 'Shopping',\n",
       "       'Auctions / Classified Ads', 'Shopping, Auctions / Classified Ads',\n",
       "       'Auctions / Classified Ads, Shopping', 'Pornography',\n",
       "       'Pornography, Erotic / Sex', 'Erotic / Sex', 'Swimwear / Lingerie',\n",
       "       'Erotic / Sex, Pornography', 'Pornography, Swimwear / Lingerie',\n",
       "       'Erotic / Sex, Swimwear / Lingerie',\n",
       "       'Swimwear / Lingerie, Pornography', 'Personal Web Sites',\n",
       "       'Education', 'Governmental Organisations', 'Political Parties',\n",
       "       'Cities / Regions / Countries, Governmental Organisations',\n",
       "       'Religion', 'Non-Governmental Organisations',\n",
       "       'Education, Governmental Organisations',\n",
       "       'Non-Governmental Organisations, Cities / Regions / Countries',\n",
       "       'Political Parties, Non-Governmental Organisations',\n",
       "       'Cities / Regions / Countries', 'Religion, Education',\n",
       "       'Governmental Organisations, Education', 'Sects',\n",
       "       'Governmental Organisations, Cities / Regions / Countries',\n",
       "       'Education, Religion', 'Education, Non-Governmental Organisations',\n",
       "       'Governmental Organisations, Education, Political Parties',\n",
       "       'Non-Governmental Organisations, Education',\n",
       "       'Political Parties, Religion',\n",
       "       'Non-Governmental Organisations, Governmental Organisations',\n",
       "       'Non-Governmental Organisations, Religion',\n",
       "       'Cities / Regions / Countries, Education',\n",
       "       'Education, Cities / Regions / Countries', 'Spam URLs',\n",
       "       'Phishing URLs', 'Malware', 'General Business', 'Cloud',\n",
       "       'Social Media, Cloud', 'Web Storage, Cloud',\n",
       "       'Banner Advertisements', 'Software as a Service, Cloud',\n",
       "       'Cloud, Web Storage', 'Social Media, Social Networking',\n",
       "       'Cloud, Software as a Service',\n",
       "       'Plattform as a Service, Cloud, Banner Advertisements',\n",
       "       'General Business, Banner Advertisements',\n",
       "       'Banner Advertisements, General Business',\n",
       "       'Software as a Service, Banner Advertisements, Cloud',\n",
       "       'Cloud, Social Networking', 'Cloud, Social Media',\n",
       "       'Banner Advertisements, Cloud, Plattform as a Service',\n",
       "       'Cloud, Plattform as a Service', 'Plattform as a Service, Cloud',\n",
       "       'Cloud, Social Networking, Social Media',\n",
       "       'Social Networking, Cloud', 'Business Networking, Cloud',\n",
       "       'Social Media', 'Software as a Service, General Business, Cloud',\n",
       "       'Plattform as a Service, General Business, Cloud',\n",
       "       'Cloud, General Business', 'Web Storage',\n",
       "       'Social Networking, Social Media, Cloud',\n",
       "       'General Business, Cloud, Web Storage', 'Social Networking',\n",
       "       'Web Storage, Social Media, Cloud',\n",
       "       'Software as a Service, Cloud, General Business',\n",
       "       'Social Media, Cloud, Web Storage',\n",
       "       'Social Networking, Cloud, Banner Advertisements',\n",
       "       'Software as a Service', 'Social Networking, Social Media',\n",
       "       'Cloud, Social Media, Web Storage',\n",
       "       'Cloud, Banner Advertisements, Software as a Service',\n",
       "       'Cloud, Software as a Service, Plattform as a Service',\n",
       "       'Plattform as a Service, Cloud, General Business',\n",
       "       'Social Media, Cloud, General Business, Social Networking',\n",
       "       'Cloud, Banner Advertisements, General Business',\n",
       "       'Business Networking, General Business',\n",
       "       'Social Media, Web Storage, Social Networking, Cloud',\n",
       "       'General Business, Plattform as a Service, Cloud',\n",
       "       'Social Media, Web Storage',\n",
       "       'Social Networking, Cloud, Social Media',\n",
       "       'General Business, Cloud',\n",
       "       'Cloud, General Business, Plattform as a Service',\n",
       "       'Cloud, General Business, Web Storage',\n",
       "       'Banner Advertisements, Cloud, Software as a Service',\n",
       "       'Cloud, Business Networking',\n",
       "       'Cloud, Social Media, Social Networking', 'Business Networking',\n",
       "       'Infrastructure as a Service, Cloud, Software as a Service, Plattform as a Service',\n",
       "       'Social Media, Cloud, Social Networking',\n",
       "       'Social Media, Social Networking, Cloud',\n",
       "       'Cloud, Plattform as a Service, Banner Advertisements',\n",
       "       'Plattform as a Service', 'Vehicles', 'Violence / Extreme',\n",
       "       'Weapons / Military'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"mon_fichier_csv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first without threads\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "def get_dataSet(file_name: str):\n",
    "    \"\"\"\n",
    "    Read a JSON file containing domain names, extract the keywords associated with each address and return a pandas Dataframe containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        file_name (str): json file \n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Panda Dataframe with extracted data\n",
    "    \"\"\"\n",
    "       \n",
    "    # creates an instance of the Scraping class   \n",
    "    scrap = Scraping()\n",
    "\n",
    "    # open the JSON file and extract the information\n",
    "    with open(file_name, \"r\") as f:\n",
    "        domaines = f.readlines()\n",
    "\n",
    "    # Initialise lists to store data\n",
    "    indices = []\n",
    "    names = []\n",
    "    categories = []\n",
    "    addresses = []\n",
    "    keywords = []\n",
    "\n",
    "    # Iterate through each domain name, extract the keywords associated with the address and store the data in the lists\n",
    "    for index, domaine in tqdm(enumerate(domaines[:50]), total=len(domaines)):\n",
    "        try:\n",
    "            dict_domaine = json.loads(domaine)\n",
    "            name = dict_domaine['name'] \n",
    "            category = dict_domaine['category']\n",
    "            address = dict_domaine['address']\n",
    "            content = dict(scrap.url_content(address))\n",
    "            \n",
    "            if content[\"lang\"] != \"None\":\n",
    "                text = clean_text(content['content_text'], content['lang'])\n",
    "                keywords.append(text)\n",
    "\n",
    "            indices.append(index)\n",
    "            names.append(name)\n",
    "            categories.append(category)\n",
    "            addresses.append(address)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction des données pour le domaine à l'index {index}: {e}\")\n",
    "\n",
    "    # Create a pandas Dataframe containing the extracted data\n",
    "    data = {'name': names, 'category': categories, 'address': addresses, 'words': keywords}\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df = get_dataSet('urls.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scrap = Scraping()\n",
    "url = \"https://cut-the-knot.org/\"\n",
    "\n",
    "content = dict(scrap.url_content(url))\n",
    "\n",
    "#print(content)\n",
    "print()\n",
    "\n",
    "if content[\"lang\"] != \"None\":\n",
    "    text = clean_text(content['content_text'], content['lang'])\n",
    "    print(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
